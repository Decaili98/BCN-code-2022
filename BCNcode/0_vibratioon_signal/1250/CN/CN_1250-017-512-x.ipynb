{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import initializers\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.regularizers import l2#正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 32768) (20, 32768) ***************************************************\n",
      "(2, 32768) (2, 32768)\n",
      "[[-0.88253   -1.3222     0.27762   ...  0.35325   -0.39045   -0.025807 ]\n",
      " [ 0.542      0.0043268  0.045142  ... -0.72797   -0.77562   -0.82809  ]] \r\n",
      " [[-0.038468 -2.3602    0.24983  ...  3.5227    1.0532    1.6303  ]\n",
      " [-0.57531  -2.2474   -1.2508   ...  0.3243   -0.36034   0.33432 ]] ***************************************************\n",
      "(1, 65536) (1, 65536)\n",
      "[[-0.88253 -1.3222   0.27762 ... -0.72797 -0.77562 -0.82809]] \r\n",
      " [[-0.038468 -2.3602    0.24983  ...  0.3243   -0.36034   0.33432 ]] ***************************************************\n"
     ]
    }
   ],
   "source": [
    "# 12-0.2\n",
    "# 13-2.4\n",
    "# 18-12.14\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "normal = np.loadtxt(r'E:\\水泵代码调试\\试验数据(包括压力脉动和振动)\\2013.9.12-未发生缠绕前\\2013-9.12振动\\2013-9-12振动-1250rmin-mat\\1250rnormalvibx.txt', delimiter=',')\n",
    "chanrao = np.loadtxt(r'E:\\水泵代码调试\\试验数据(包括压力脉动和振动)\\2013.9.17-发生缠绕后\\振动\\9-18上午振动1250rmin-mat\\1250r_chanraovibx.txt', delimiter=',')\n",
    "print(normal.shape,chanrao.shape,\"***************************************************\")\n",
    "data_normal=normal[10:12]   #提取前两行\n",
    "data_chanrao=chanrao[10:12]   #提取前两行\n",
    "print(data_normal.shape,data_chanrao.shape)\n",
    "print(data_normal,\"\\r\\n\",data_chanrao,\"***************************************************\")\n",
    "data_normal=data_normal.reshape(1,-1)\n",
    "data_chanrao=data_chanrao.reshape(1,-1)\n",
    "print(data_normal.shape,data_chanrao.shape)\n",
    "print(data_normal,\"\\r\\n\",data_chanrao,\"***************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 512) (128, 512)\n"
     ]
    }
   ],
   "source": [
    "#水泵的两种故障类型信号normal正常，chanrao故障\n",
    "data_normal=data_normal.reshape(-1, 512)#(65536,1)-(128, 515)\n",
    "data_chanrao=data_chanrao.reshape(-1,512)\n",
    "print(data_normal.shape,data_chanrao.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204, 512, 1) (52, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def yuchuli(data,label):#(4:1)(51:13)\n",
    "    #打乱数据顺序\n",
    "    np.random.shuffle(data)\n",
    "    train = data[0:102,:]\n",
    "    test = data[102:128,:]\n",
    "    label_train = np.array([label for i in range(0,102)])\n",
    "    label_test =np.array([label for i in range(0,26)])\n",
    "    return train,test ,label_train ,label_test\n",
    "def stackkk(a,b,c,d,e,f,g,h):\n",
    "    aa = np.vstack((a, e))\n",
    "    bb = np.vstack((b, f))\n",
    "    cc = np.hstack((c, g))\n",
    "    dd = np.hstack((d, h))\n",
    "    return aa,bb,cc,dd\n",
    "x_tra0,x_tes0,y_tra0,y_tes0 = yuchuli(data_normal,0)\n",
    "x_tra1,x_tes1,y_tra1,y_tes1 = yuchuli(data_chanrao,1)\n",
    "tr1,te1,yr1,ye1=stackkk(x_tra0,x_tes0,y_tra0,y_tes0 ,x_tra1,x_tes1,y_tra1,y_tes1)\n",
    "\n",
    "x_train=tr1\n",
    "x_test=te1\n",
    "y_train = yr1\n",
    "y_test = ye1\n",
    "\n",
    "#打乱数据\n",
    "state = np.random.get_state()\n",
    "np.random.shuffle(x_train)\n",
    "np.random.set_state(state)\n",
    "np.random.shuffle(y_train)\n",
    "\n",
    "state = np.random.get_state()\n",
    "np.random.shuffle(x_test)\n",
    "np.random.set_state(state)\n",
    "np.random.shuffle(y_test)\n",
    "\n",
    "\n",
    "#对训练集和测试集标准化\n",
    "def ZscoreNormalization(x):\n",
    "    \"\"\"Z-score normaliaztion\"\"\"\n",
    "    x = (x - np.mean(x)) / np.std(x)\n",
    "    return x\n",
    "x_train=ZscoreNormalization(x_train)\n",
    "x_test=ZscoreNormalization(x_test)\n",
    "# print(x_test[0])\n",
    "\n",
    "\n",
    "#转化为一维序列\n",
    "x_train = x_train.reshape(-1,512,1)\n",
    "x_test = x_test.reshape(-1,512,1)\n",
    "print(x_train.shape,x_test.shape)\n",
    "\n",
    "def to_one_hot(labels,dimension=2):\n",
    "    results = np.zeros((len(labels),dimension))\n",
    "    for i,label in enumerate(labels):\n",
    "        results[i,label] = 1\n",
    "    return results\n",
    "one_hot_train_labels = to_one_hot(y_train)\n",
    "one_hot_test_labels = to_one_hot(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义挤压函数\n",
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    对向量的非线性激活函数\n",
    "    ## vectors: some vectors to be squashed, N-dim tensor\n",
    "    ## axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    计算向量的长度。它用于计算与margin_loss中的y_true具有相同形状的张量\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n",
    "    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n",
    "    output: shape=[dim_1, ..., dim_{n-1}]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "         return input_shape[:-1]\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super(Length, self).get_config()\n",
    "        return config\n",
    "#定义预胶囊层\n",
    "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    进行普通二维卷积 `n_channels` 次, 然后将所有的胶囊重叠起来\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_capsule: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
    "    \"\"\"\n",
    "    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides,\n",
    "                           padding=padding,name='primarycap_conv2d')(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)\n",
    "\n",
    "class DenseCapsule(layers.Layer):\n",
    "    \"\"\"\n",
    "    胶囊层. 输入输出都为向量. \n",
    "    ## num_capsule: 本层包含的胶囊数量\n",
    "    ## dim_capsule: 输出的每一个胶囊向量的维度\n",
    "    ## routings: routing 算法的迭代次数\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_initializer='glorot_uniform',**kwargs):\n",
    "        super(DenseCapsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, '输入的 Tensor 的形状[None, input_num_capsule, input_dim_capsule]'#(None,1152,8)\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "\n",
    "        #转换矩阵\n",
    "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
    "                                        self.dim_capsule, self.input_dim_capsule],\n",
    "                                initializer=self.kernel_initializer,name='W')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsuie, input_dim_capsule]\n",
    "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
    "        inputs_expand = K.expand_dims(inputs, 1)\n",
    "        # 运算优化:将inputs_expand重复num_capsule 次，用于快速和W相乘\n",
    "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
    "\n",
    "        # 将inputs_tiled的batch中的每一条数据，计算inputs+W\n",
    "        # x.shape = [num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        # W.shape = [num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
    "        # 将x和W的前两个维度看作'batch'维度，向量和矩阵相乘:\n",
    "        # [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
    "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsutel\n",
    "        inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]),elems=inputs_tiled)\n",
    "\n",
    "        # Begin: Routing算法\n",
    "        # 将系数b初始化为0.\n",
    "        # b.shape = [None, self.num_capsule, self, input_num_capsule].\n",
    "        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n",
    "        \n",
    "        assert self.routings > 0, 'The routings should be > 0.'\n",
    "        for i in range(self.routings):\n",
    "            # c.shape=[None, num_capsule, input_num_capsule]\n",
    "            C = tf.nn.softmax(b ,axis=1)\n",
    "            # c.shape = [None, num_capsule, input_num_capsule]\n",
    "            # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "            # 将c与inputs_hat的前两个维度看作'batch'维度，向量和矩阵相乘:\n",
    "            # [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule],\n",
    "            # outputs.shape= [None, num_capsule, dim_capsule]\n",
    "            outputs = squash(K. batch_dot(C, inputs_hat, [2, 2])) # [None, 10, 16]\n",
    "        \n",
    "            if i < self.routings - 1:\n",
    "                # outputs.shape = [None, num_capsule, dim_capsule]\n",
    "                # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # 将outputs和inρuts_hat的前两个维度看作‘batch’ 维度，向量和矩阵相乘:\n",
    "                # [dim_capsule] x [imput_num_capsule, dim_capsule]^T -> [input_num_capsule]\n",
    "                # b.shape = [batch_size. num_capsule, input_nom_capsule]\n",
    "#                 b += K.batch_dot(outputs, inputs_hat, [2, 3]) to this b += tf.matmul(self.W, x)\n",
    "                b += K.batch_dot(outputs, inputs_hat, [2, 3])\n",
    "\n",
    "        # End: Routing 算法\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_capsule': self.num_capsule,\n",
    "            'dim_capsule': self.dim_capsule,\n",
    "            'routings': self.routings\n",
    "            }\n",
    "        base_config = super(DenseCapsule, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anaconda0\\envs\\tf2.4\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 512, 1, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 511, 1, 16)        48        \n",
      "_________________________________________________________________\n",
      "primarycap_conv2d (Conv2D)   (None, 254, 1, 96)        6240      \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 3048, 8)           0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 3048, 8)           0         \n",
      "_________________________________________________________________\n",
      "digit_caps (DenseCapsule)    (None, 2, 16)             780288    \n",
      "_________________________________________________________________\n",
      "out_caps (Length)            (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 786,576\n",
      "Trainable params: 786,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.regularizers import l2#正则化\n",
    "x = layers.Input(shape=[512,1, 1])\n",
    "#普通卷积层\n",
    "conv1 = layers.Conv2D(filters=16, kernel_size=(2, 1),activation='relu',padding='valid',name='conv1')(x)\n",
    "\n",
    "# Layer 3: 使用“squash”激活的Conv2D层， 然后重塑 [None, num_capsule, dim_vector]\n",
    "primarycaps = PrimaryCap(conv1, dim_capsule=8, n_channels=12, kernel_size=(4, 1), strides=2, padding='valid')\n",
    "# Layer 4: 数字胶囊层，动态路由算法在这里工作。\n",
    "digitcaps = DenseCapsule(num_capsule=2, dim_capsule=16, routings=3, name='digit_caps')(primarycaps)\n",
    "# Layer 5:这是一个辅助层，用它的长度代替每个胶囊。只是为了符合标签的形状。\n",
    "out_caps = Length(name='out_caps')(digitcaps)\n",
    "\n",
    "model = keras.Model(x, out_caps)    \n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#定义优化\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19/19 [==============================] - 7s 192ms/step - loss: 0.6377 - accuracy: 0.5625 - val_loss: 0.6853 - val_accuracy: 0.3333\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 0.4506 - accuracy: 0.5489 - val_loss: 0.5556 - val_accuracy: 0.3333\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 0.4258 - accuracy: 0.4810 - val_loss: 0.5058 - val_accuracy: 0.3333\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 0.3666 - accuracy: 0.6718 - val_loss: 0.4439 - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 2s 105ms/step - loss: 0.2919 - accuracy: 1.0000 - val_loss: 0.1301 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 2s 105ms/step - loss: 0.1013 - accuracy: 1.0000 - val_loss: 0.0807 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 2s 84ms/step - loss: 0.0593 - accuracy: 1.0000 - val_loss: 0.0803 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 2s 83ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.0600 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 0.0505 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 1s 74ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0422 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 1s 77ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0413 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 2s 113ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0390 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 1s 77ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 9.9724e-04 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 4.6235e-04 - accuracy: 1.0000 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 2.3352e-04 - accuracy: 1.0000 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 1.4725e-04 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 2.4550e-04 - accuracy: 1.0000 - val_loss: 0.0392 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 1.5240e-04 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 9.0397e-05 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 5.8291e-05 - accuracy: 1.0000 - val_loss: 0.0392 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 2s 88ms/step - loss: 3.5282e-05 - accuracy: 1.0000 - val_loss: 0.0398 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 2s 88ms/step - loss: 2.2559e-05 - accuracy: 1.0000 - val_loss: 0.0396 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 1.3768e-05 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 5.9496e-05 - accuracy: 1.0000 - val_loss: 0.0394 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 2s 79ms/step - loss: 9.5306e-05 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 1.3143e-04 - accuracy: 1.0000 - val_loss: 0.0399 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 8.2964e-05 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 2s 79ms/step - loss: 5.7560e-05 - accuracy: 1.0000 - val_loss: 0.0395 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 1s 77ms/step - loss: 3.4184e-05 - accuracy: 1.0000 - val_loss: 0.0385 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 1s 77ms/step - loss: 4.8030e-05 - accuracy: 1.0000 - val_loss: 0.0394 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      " 8/19 [===========>..................] - ETA: 0s - loss: 3.0926e-05 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time_begin = time.time()\n",
    "history = model.fit(x_train,one_hot_train_labels,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=50,batch_size=10,\n",
    "                    shuffle=True)\n",
    "time_end = time.time()\n",
    "time = time_end - time_begin\n",
    "print('time:', time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time_begin = time.time()\n",
    "score = model.evaluate(x_test,one_hot_test_labels, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    " \n",
    "time_end = time.time()\n",
    "time = time_end - time_begin\n",
    "print('time:', time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#绘制acc-loss曲线\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'],color='r')\n",
    "plt.plot(history.history['val_loss'],color='g')\n",
    "plt.plot(history.history['accuracy'],color='b')\n",
    "plt.plot(history.history['val_accuracy'],color='k')\n",
    "plt.title('model loss and acc')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'test_loss','train_acc', 'test_acc'], loc='center right')\n",
    "# plt.legend(['train_loss','train_acc'], loc='upper left')\n",
    "#plt.savefig('1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'],color='r')\n",
    "plt.plot(history.history['accuracy'],color='b')\n",
    "plt.title('model loss and sccuracy ')\n",
    "plt.ylabel('loss/sccuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'train_sccuracy'], loc='center right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
