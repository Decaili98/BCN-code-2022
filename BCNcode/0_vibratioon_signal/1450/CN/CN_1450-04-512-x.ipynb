{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import initializers\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.regularizers import l2#正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 32768) (22, 32768) ***************************************************\n",
      "(2, 32768) (2, 32768)\n",
      "[[ 0.0026748 -0.86509   -0.19289   ...  1.9146     1.5985     1.4293   ]\n",
      " [-0.11676   -2.1445    -0.14518   ... -0.49074    0.26571    0.47469  ]] \r\n",
      " [[ 1.1713  -0.32797 -2.8769  ... -2.1529   0.57767 -0.60373]\n",
      " [-1.4074  -1.4745   1.3035  ...  1.2364   0.54452  0.62937]] ***************************************************\n",
      "(1, 65536) (1, 65536)\n",
      "[[ 0.0026748 -0.86509   -0.19289   ... -0.49074    0.26571    0.47469  ]] \r\n",
      " [[ 1.1713  -0.32797 -2.8769  ...  1.2364   0.54452  0.62937]] ***************************************************\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "normal = np.loadtxt(r'E:\\水泵代码调试\\试验数据(包括压力脉动和振动)\\2013.9.12-未发生缠绕前\\2013-9.12振动\\2013-9-12振动-1450rmin-mat\\1450r_normalvibx.txt', delimiter=',')\n",
    "chanrao = np.loadtxt(r'E:\\水泵代码调试\\试验数据(包括压力脉动和振动)\\2013.9.17-发生缠绕后\\振动\\9-17下午振动1450rmin-mat\\1450r_chanraovibx.txt', delimiter=',')\n",
    "print(normal.shape,chanrao.shape,\"***************************************************\")\n",
    "data_normal=normal[6:8]   #提取前两行\n",
    "data_chanrao=chanrao[6:8]   #提取前两行\n",
    "print(data_normal.shape,data_chanrao.shape)\n",
    "print(data_normal,\"\\r\\n\",data_chanrao,\"***************************************************\")\n",
    "data_normal=data_normal.reshape(1,-1)\n",
    "data_chanrao=data_chanrao.reshape(1,-1)\n",
    "print(data_normal.shape,data_chanrao.shape)\n",
    "print(data_normal,\"\\r\\n\",data_chanrao,\"***************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 512) (128, 512)\n"
     ]
    }
   ],
   "source": [
    "#水泵的两种故障类型信号normal正常，chanrao故障\n",
    "data_normal=data_normal.reshape(-1, 512)#(65536,1)-(128, 515)\n",
    "data_chanrao=data_chanrao.reshape(-1,512)\n",
    "print(data_normal.shape,data_chanrao.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204, 512, 1) (52, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def yuchuli(data,label):#(4:1)(51:13)\n",
    "    #打乱数据顺序\n",
    "    np.random.shuffle(data)\n",
    "    train = data[0:102,:]\n",
    "    test = data[102:128,:]\n",
    "    label_train = np.array([label for i in range(0,102)])\n",
    "    label_test =np.array([label for i in range(0,26)])\n",
    "    return train,test ,label_train ,label_test\n",
    "def stackkk(a,b,c,d,e,f,g,h):\n",
    "    aa = np.vstack((a, e))\n",
    "    bb = np.vstack((b, f))\n",
    "    cc = np.hstack((c, g))\n",
    "    dd = np.hstack((d, h))\n",
    "    return aa,bb,cc,dd\n",
    "x_tra0,x_tes0,y_tra0,y_tes0 = yuchuli(data_normal,0)\n",
    "x_tra1,x_tes1,y_tra1,y_tes1 = yuchuli(data_chanrao,1)\n",
    "tr1,te1,yr1,ye1=stackkk(x_tra0,x_tes0,y_tra0,y_tes0 ,x_tra1,x_tes1,y_tra1,y_tes1)\n",
    "\n",
    "x_train=tr1\n",
    "x_test=te1\n",
    "y_train = yr1\n",
    "y_test = ye1\n",
    "\n",
    "#打乱数据\n",
    "state = np.random.get_state()\n",
    "np.random.shuffle(x_train)\n",
    "np.random.set_state(state)\n",
    "np.random.shuffle(y_train)\n",
    "\n",
    "state = np.random.get_state()\n",
    "np.random.shuffle(x_test)\n",
    "np.random.set_state(state)\n",
    "np.random.shuffle(y_test)\n",
    "\n",
    "\n",
    "#对训练集和测试集标准化\n",
    "def ZscoreNormalization(x):\n",
    "    \"\"\"Z-score normaliaztion\"\"\"\n",
    "    x = (x - np.mean(x)) / np.std(x)\n",
    "    return x\n",
    "x_train=ZscoreNormalization(x_train)\n",
    "x_test=ZscoreNormalization(x_test)\n",
    "# print(x_test[0])\n",
    "\n",
    "\n",
    "#转化为一维序列\n",
    "x_train = x_train.reshape(-1,512,1)\n",
    "x_test = x_test.reshape(-1,512,1)\n",
    "print(x_train.shape,x_test.shape)\n",
    "\n",
    "def to_one_hot(labels,dimension=2):\n",
    "    results = np.zeros((len(labels),dimension))\n",
    "    for i,label in enumerate(labels):\n",
    "        results[i,label] = 1\n",
    "    return results\n",
    "one_hot_train_labels = to_one_hot(y_train)\n",
    "one_hot_test_labels = to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义挤压函数\n",
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    对向量的非线性激活函数\n",
    "    ## vectors: some vectors to be squashed, N-dim tensor\n",
    "    ## axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    计算向量的长度。它用于计算与margin_loss中的y_true具有相同形状的张量\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n",
    "    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n",
    "    output: shape=[dim_1, ..., dim_{n-1}]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "         return input_shape[:-1]\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super(Length, self).get_config()\n",
    "        return config\n",
    "#定义预胶囊层\n",
    "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    进行普通二维卷积 `n_channels` 次, 然后将所有的胶囊重叠起来\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_capsule: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
    "    \"\"\"\n",
    "    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides,\n",
    "                           padding=padding,name='primarycap_conv2d')(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)\n",
    "\n",
    "class DenseCapsule(layers.Layer):\n",
    "    \"\"\"\n",
    "    胶囊层. 输入输出都为向量. \n",
    "    ## num_capsule: 本层包含的胶囊数量\n",
    "    ## dim_capsule: 输出的每一个胶囊向量的维度\n",
    "    ## routings: routing 算法的迭代次数\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_initializer='glorot_uniform',**kwargs):\n",
    "        super(DenseCapsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, '输入的 Tensor 的形状[None, input_num_capsule, input_dim_capsule]'#(None,1152,8)\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "\n",
    "        #转换矩阵\n",
    "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
    "                                        self.dim_capsule, self.input_dim_capsule],\n",
    "                                initializer=self.kernel_initializer,name='W')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsuie, input_dim_capsule]\n",
    "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
    "        inputs_expand = K.expand_dims(inputs, 1)\n",
    "        # 运算优化:将inputs_expand重复num_capsule 次，用于快速和W相乘\n",
    "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
    "\n",
    "        # 将inputs_tiled的batch中的每一条数据，计算inputs+W\n",
    "        # x.shape = [num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        # W.shape = [num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
    "        # 将x和W的前两个维度看作'batch'维度，向量和矩阵相乘:\n",
    "        # [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
    "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsutel\n",
    "        inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]),elems=inputs_tiled)\n",
    "\n",
    "        # Begin: Routing算法\n",
    "        # 将系数b初始化为0.\n",
    "        # b.shape = [None, self.num_capsule, self, input_num_capsule].\n",
    "        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n",
    "        \n",
    "        assert self.routings > 0, 'The routings should be > 0.'\n",
    "        for i in range(self.routings):\n",
    "            # c.shape=[None, num_capsule, input_num_capsule]\n",
    "            C = tf.nn.softmax(b ,axis=1)\n",
    "            # c.shape = [None, num_capsule, input_num_capsule]\n",
    "            # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "            # 将c与inputs_hat的前两个维度看作'batch'维度，向量和矩阵相乘:\n",
    "            # [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule],\n",
    "            # outputs.shape= [None, num_capsule, dim_capsule]\n",
    "            outputs = squash(K. batch_dot(C, inputs_hat, [2, 2])) # [None, 10, 16]\n",
    "        \n",
    "            if i < self.routings - 1:\n",
    "                # outputs.shape = [None, num_capsule, dim_capsule]\n",
    "                # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # 将outputs和inρuts_hat的前两个维度看作‘batch’ 维度，向量和矩阵相乘:\n",
    "                # [dim_capsule] x [imput_num_capsule, dim_capsule]^T -> [input_num_capsule]\n",
    "                # b.shape = [batch_size. num_capsule, input_nom_capsule]\n",
    "#                 b += K.batch_dot(outputs, inputs_hat, [2, 3]) to this b += tf.matmul(self.W, x)\n",
    "                b += K.batch_dot(outputs, inputs_hat, [2, 3])\n",
    "\n",
    "        # End: Routing 算法\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_capsule': self.num_capsule,\n",
    "            'dim_capsule': self.dim_capsule,\n",
    "            'routings': self.routings\n",
    "            }\n",
    "        base_config = super(DenseCapsule, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anaconda0\\envs\\tf2.4\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 512, 1, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 511, 1, 16)        48        \n",
      "_________________________________________________________________\n",
      "primarycap_conv2d (Conv2D)   (None, 254, 1, 96)        6240      \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 3048, 8)           0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 3048, 8)           0         \n",
      "_________________________________________________________________\n",
      "digit_caps (DenseCapsule)    (None, 2, 16)             780288    \n",
      "_________________________________________________________________\n",
      "out_caps (Length)            (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 786,576\n",
      "Trainable params: 786,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.regularizers import l2#正则化\n",
    "x = layers.Input(shape=[512,1, 1])\n",
    "#普通卷积层\n",
    "conv1 = layers.Conv2D(filters=16, kernel_size=(2, 1),activation='relu',padding='valid',name='conv1')(x)\n",
    "\n",
    "# Layer 3: 使用“squash”激活的Conv2D层， 然后重塑 [None, num_capsule, dim_vector]\n",
    "primarycaps = PrimaryCap(conv1, dim_capsule=8, n_channels=12, kernel_size=(4, 1), strides=2, padding='valid')\n",
    "# Layer 4: 数字胶囊层，动态路由算法在这里工作。\n",
    "digitcaps = DenseCapsule(num_capsule=2, dim_capsule=16, routings=3, name='digit_caps')(primarycaps)\n",
    "# Layer 5:这是一个辅助层，用它的长度代替每个胶囊。只是为了符合标签的形状。\n",
    "out_caps = Length(name='out_caps')(digitcaps)\n",
    "\n",
    "model = keras.Model(x, out_caps)    \n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19/19 [==============================] - 7s 203ms/step - loss: 0.1605 - accuracy: 0.5099 - val_loss: 0.0892 - val_accuracy: 0.4762\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.0807 - accuracy: 0.5677 - val_loss: 0.0524 - val_accuracy: 0.8095\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 2s 117ms/step - loss: 0.0311 - accuracy: 0.9119 - val_loss: 0.0188 - val_accuracy: 0.9524\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 0.9524\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 2s 100ms/step - loss: 8.6381e-04 - accuracy: 1.0000 - val_loss: 0.0176 - val_accuracy: 0.9524\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 4.0194e-04 - accuracy: 1.0000 - val_loss: 0.0169 - val_accuracy: 0.9524\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 1s 78ms/step - loss: 1.1673e-04 - accuracy: 1.0000 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 2s 84ms/step - loss: 2.0751e-04 - accuracy: 1.0000 - val_loss: 0.0180 - val_accuracy: 0.9524\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 2.6118e-05 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 1s 77ms/step - loss: 1.3139e-05 - accuracy: 1.0000 - val_loss: 0.0176 - val_accuracy: 0.9524\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 2s 80ms/step - loss: 1.0601e-05 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 0.9524\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 1.8048e-06 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 0.9524\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 1.6430e-06 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 0.9524\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 1s 77ms/step - loss: 1.0761e-06 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 0.9524\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 1.0877e-06 - accuracy: 1.0000 - val_loss: 0.0185 - val_accuracy: 0.9524\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 1s 75ms/step - loss: 4.7885e-07 - accuracy: 1.0000 - val_loss: 0.0186 - val_accuracy: 0.9524\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 1s 72ms/step - loss: 1.6300e-07 - accuracy: 1.0000 - val_loss: 0.0186 - val_accuracy: 0.9524\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 1s 72ms/step - loss: 1.1869e-07 - accuracy: 1.0000 - val_loss: 0.0187 - val_accuracy: 0.9524\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 1s 71ms/step - loss: 8.0907e-08 - accuracy: 1.0000 - val_loss: 0.0187 - val_accuracy: 0.9524\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 1s 73ms/step - loss: 3.1394e-07 - accuracy: 1.0000 - val_loss: 0.0187 - val_accuracy: 0.9524\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 1s 76ms/step - loss: 1.0255e-07 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 0.9524\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 5.5112e-08 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 0.9524loss: 4.8760e-08 - accuracy: \n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 2.1701e-08 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 0.9524\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 5.5988e-08 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 0.9524\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 1s 78ms/step - loss: 1.3940e-08 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 0.9524\n",
      "Epoch 26/50\n",
      "13/19 [===================>..........] - ETA: 0s - loss: 7.6731e-09 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a2cda4602855>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtime_begin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m history = model.fit(x_train,one_hot_train_labels,\n\u001b[0m\u001b[0;32m      9\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda0\\envs\\tf2.4\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda0\\envs\\tf2.4\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda0\\envs\\tf2.4\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda0\\envs\\tf2.4\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda0\\envs\\tf2.4\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mE:\\anaconda0\\envs\\tf2.4\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda0\\envs\\tf2.4\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#定义优化\n",
    "model.compile(metrics=['accuracy'],\n",
    "              optimizer='adam',\n",
    "              loss=lambda y_true,y_pred: y_true*K.relu(0.9-y_pred)**2 + 0.25*(1-y_true)*K.relu(y_pred-0.1)**2   \n",
    "             )\n",
    "import time\n",
    "time_begin = time.time()\n",
    "history = model.fit(x_train,one_hot_train_labels,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=50,batch_size=10,\n",
    "                    shuffle=True)\n",
    "time_end = time.time()\n",
    "time = time_end - time_begin\n",
    "print('time:', time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test,one_hot_test_labels, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#绘制acc-loss曲线\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'],color='r')\n",
    "plt.plot(history.history['val_loss'],color='g')\n",
    "plt.plot(history.history['accuracy'],color='b')\n",
    "plt.plot(history.history['val_accuracy'],color='k')\n",
    "plt.title('model loss and acc')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'test_loss','train_acc', 'test_acc'], loc='upper left')\n",
    "# plt.legend(['train_loss','train_acc'], loc='upper left')\n",
    "#plt.savefig('1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'],color='r')\n",
    "plt.plot(history.history['accuracy'],color='b')\n",
    "plt.title('model loss and sccuracy ')\n",
    "plt.ylabel('loss/sccuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'train_sccuracy'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
